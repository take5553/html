# Image Classification

コードは大きく分けて

* Interactive Form（iPython widget）
* Neural Network（PyTorch）

に分かれる。

## Interactive Form

### 全体像

まず以下をインポート。

~~~python
import ipywidgets
~~~

そして、色々した後にできるものが以下。（緑の注釈は公式によるもの）

![image-20210702220220775](image/03/image-20210702220220775.png)

~~~python
from IPython.display import display

(略)

# Combine all the widgets into one display
all_widget = ipywidgets.VBox([
    ipywidgets.HBox([data_collection_widget, live_execution_widget]), 
    train_eval_widget,
    model_widget
])

display(all_widget)
~~~

赤枠が`ipywidgets.VBox`の中身で

* `ipywidgets.HBox`（青枠）→`data_collection_widget`、`live_execution_widget`
* `train_eval_widget`
* `model_widget`

の順で並ぶ。

それぞれのWidgetはこのコードの前にすでに作っておく。

最後に`display`で`ipywidgets.VBox`を表示すれば良い。

### `data_collection_widget`

全体像。

![image-20210702221319634](image/03/image-20210702221319634.png)

大きな`VBox`の中にいくつかのWidget（赤枠）が入れられ、一つのWidgetの塊を作っている。

~~~python
(略)

# create image preview
camera_widget = ipywidgets.Image()

(略)

# create widgets
dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')
category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')
count_widget = ipywidgets.IntText(description='count')
save_widget = ipywidgets.Button(description='add')

(略)

data_collection_widget = ipywidgets.VBox([
    ipywidgets.HBox([camera_widget]), dataset_widget, category_widget, count_widget, save_widget
])
~~~

#### カメラについて

~~~python
(略)

from jetcam.usb_camera import USBCamera

(略)

# for USB Camera (Logitech C270 webcam), uncomment the following line
camera = USBCamera(width=224, height=224, capture_device=0) # confirm the capture_device number

(略)

camera.running = True

(略)

import traitlets
from jetcam.utils import bgr8_to_jpeg

camera.unobserve_all()

# create image preview
camera_widget = ipywidgets.Image()
traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)
~~~

* `jetcam`・・・Jetson Nanoでカメラを使う用に[NVIDIAが作ったライブラリ](https://github.com/NVIDIA-AI-IOT/jetcam)っぽい。

  * `camera = USBCamera(...)`：カメラインスタンスを生成

  * `image = camera.read()`：カメラで写真を撮る

    （実際はOpenCVの`VideoCapture`クラスの`read()`メソッドで取り込んでいるっぽいので、`image`は`numpy.ndarray`）

  * ~~~python
    image_widget = ipywidgets.Image(format='jpeg')
    image_widget.value = bgr8_to_jpeg(image)
    display(image_widget)
    ~~~

    こうしてやることで取り込んだ写真を表示することができる。`bgr8_to_jpeg`は`jetcam`内にある。実際は`cv2.imencode`を使ってJPEGに変換しているらしい。

  * `camera.running = true`

    これはUSBカメラの画像を連続的に（具体的には`while True`ループで）`read()`を実行する

  * `camera.value`：読み込んだ画像が収められる場所。`running = true`だと連続的に値が変化する。

* `traitlets`・・・クラスのメンバ変数のデフォルト値を決めたり、バリデーションしたり、値の変更を監視したりするライブラリ。`jetcam.USBCamera`内でも使われている。

  * `camera.observe(関数, names=メンバ変数名)`：メンバ変数の値が変わったら関数をコールバックするようにセットする。

    ~~~python
    camera.running = true
    camera.observe(関数, names=value)
    ~~~

    とすると、`value`は`running = true`によって常に変化するので関数がループ的に常に実行されることになる。

    * 関数には引数として値の変化に関する辞書（`new`とか`old`とかをキーとする辞書）が渡されて実行されるらしい。ということは逆に関数の定義にはそれ用の引数を用意しておかないといけない。

  * `unobserve_all()`：名前から察するに全ての`observe`を解除する（つまり監視を止める）ということかな。
  * `dlink((クラス1, メンバ変数名), (クラス2, メンバ変数名), transform=関数)`：クラス1の指定メンバ変数が変更されたときに、クラス2のメンバ変数にも同じ値（または`transform`で指定した関数を通した値）をセットする。
    * クラス1→クラス2という方向があり、逆は無い
    * ソースコードによると`dlink`は`directional_link`のことらしい。
    * 双方向で同期させたかったら`link`というメソッドを使う。

#### ドロップダウン`dataset_widget`について

~~~python
import torchvision.transforms as transforms
from dataset import ImageClassificationDataset

TASK = 'thumbs'
# TASK = 'emotions'
# TASK = 'fingers'
# TASK = 'diy'

CATEGORIES = ['thumbs_up', 'thumbs_down']
# CATEGORIES = ['none', 'happy', 'sad', 'angry']
# CATEGORIES = ['1', '2', '3', '4', '5']
# CATEGORIES = [ 'diy_1', 'diy_2', 'diy_3']

DATASETS = ['A', 'B']
# DATASETS = ['A', 'B', 'C']

TRANSFORMS = transforms.Compose([
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

datasets = {}
for name in DATASETS:
    datasets[name] = ImageClassificationDataset('../data/classification/' + TASK + '_' + name, CATEGORIES, TRANSFORMS)

(略)

dataset = datasets[DATASETS[0]]

(略)

dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')

(略)

def set_dataset(change):
    global dataset
    dataset = datasets[change['new']]
    count_widget.value = dataset.get_count(category_widget.value)
dataset_widget.observe(set_dataset, names='value')
~~~

生成は`dataset_widget = ipywidgets.Dropdown(options=リスト, description=ラベル)`という書式。

* 引数として何が渡せるのかは`dataset_widget.keys`と書いて実行してみるとわかる。あとは[公式ドキュメント](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html#Dropdown)を見て頑張る。

  ~~~python
  dataset_widget.keys
  
  ['_dom_classes',
   '_model_module',
   '_model_module_version',
   '_model_name',
   '_options_labels',
   '_view_count',
   '_view_module',
   '_view_module_version',
   '_view_name',
   'description',
   'description_tooltip',
   'disabled',
   'index',
   'layout',
   'style']
  ~~~

* [Widget Events — Jupyter Widgets 7.6.2 documentation](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Events.html#Traitlet-events)にも書いてあるとおり、`traitlets`の`observe`が使える。

* `dataset_widget.value`が現在選択している項目。

## 参考

[非エンジニアでも使いやすい高機能なPython環境「IPython」「Jupyter」を使ってみよう | さくらのナレッジ](https://knowledge.sakura.ad.jp/17727/)
[GitHub - NVIDIA-AI-IOT/jetcam: Easy to use Python camera interface for NVIDIA Jetson](https://github.com/NVIDIA-AI-IOT/jetcam)
[Python, OpenCVで動画を読み込み（ファイル・カメラ映像） | note.nkmk.me](https://note.nkmk.me/python-opencv-videocapture-file-camera/)
[jupyterを支える技術：traitlets（の解読を試みようとした話） - Qiita](https://qiita.com/cotton-gluon/items/c343a1f57e46e3238a32)
[traitlets/traitlets.py at main · ipython/traitlets · GitHub](https://github.com/ipython/traitlets/blob/main/traitlets/traitlets.py)
